\chapter{Background}
\label{ch:background}

\section{Reinforcement learning}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=8cm]{images/ReinforcementLearning}
    \label{fig:rlearn}
    \caption{Simplified reinforcement learning model}
\end{figure}
Figure~\ref{fig:rlearn} illustrates our simplified model of reinforcement learning.
In this model, on every iteration the agent takes an action and receives a reward from the environment based on that action.
This feedback is then used by the agent to learn about the environment.
By repeatedly choosing actions and receiving feedback from the environment, the agent is able to decide which action is the most profitable.
The more general version of reinforcement learning includes a communication of the environment's state to the agent, but in our case the environment does not change state and therefore this particularity is omitted from the figure.
A comprehensive survey of reinforcement learning is found in \cite{Kaelbling1996}.

The goal of the agent is to maximise the rewards received in the long term.
This performance measure can equivalently be defined as minimising \emph{regret}, that is, minimising the difference between actual received rewards and the rewards that arise from always choosing the optimal action.
We talk of two types of regret:
Total regret $R_T$, where we consider the total sum of rewards, and instantaneous regret $R_I(t)$, where we consider the reward received at a specific point in time.
\begin{align*}
    R_I(t) &= \mu^* - r_t \\
    R_T &= T\mu^* - \sum_{t=1}^{T}{r_t} = \sum_{t = 1}^{T}{R_I(t)} 
\end{align*}

Given that the environment normally is considered to be stochastic, the agent has to weigh exploration against exploration.
Actions have variable rewards associated with them, and exploration is needed to estimate the real expected payoff of each action, but if the agent allocates too much or too little time for exploration, its total reward suffers.
Thus the agent has to decide on whether to continue exploration -- to get a clearer picture of which action is the optimal one -- or to exploit the knowledge it already has acquired and select the action it currently believes to be the best one.

\section{The multi-armed bandit problem}
The simplest \cite{Kaelbling1996} way to model reinforcement learning problems is through the multi-armed bandit scenario.
Imagine the iconic gambling machine found in any self-respecting casino, only this version of the apparatus has more than one arm with which a player may try her luck.
The arms may cause the machine to give the player different rewards, so she will want to choose the arm which gives the highest payoff.
Hence, a strategy for optimising the machine's payout must be employed.

Before any arm-pulling has been done, nothing is known about the possible payoffs from the available arms except that some arms are better than others -- obviously our player will profit the most from playing only the best arm.
And now we again see the dilemma of exploration and exploitation: she must make a trade-off between exploration 
-- discovering which arm is truly the best one -- and exploitation -- actually picking the best arm.
When selecting to explore the short-term reward is sacrificed for possible long-term payoffs, while exploitation may cause the player to not achieve the best reward possible due to incomplete knowledge of the search space.

In this paper we look at Gaussian bandits, as opposed to the more common Bernoulli bandits.
This entails that arms are normally distributed, and each arm is described simply by a mean and variance.
The choice of Gaussian distributions is also due to the fact that the Thompson sampling algorithm requires distributions with conjugate priors -- the normal distribution is self-conjugate.


\section{The Goore Game}
Introduced by Tsetlin \cite{Tsetlin1964} in 1964, the Goore game\footnote{Also known as the Gur game.} is an iterated cooperation game where each player is not privy to information on the other players.
It can be described as a voting process where the referee knows the correct answer, and the players cast their votes as simple ‘yes’ or ‘no’.
The correct answer is a certain fraction of yes-votes, and players are rewarded based on how close they collectively are to the ideal solution.
Rewards are determined by a unimodal performance criterion $G(\lambda)$, for instance the density of the normal distribution, where $\lambda$ is the fraction of yes-votes.
The fraction of yes-votes preferred by the referee corresponds to the maximum of the performance criterion; when the normal distribution is used it is simply the mean value of the distribution. 

The Goore game is inherently noisy, in that the reward for a single player is dependent on the actions of all the other players, resulting in reward variance.
The noise level is at its highest as $\lambda = 0.5$.
At this point the reward received by one player is severely dependent upon the other players choices, and so has the highest variance. 
Conversely, the lowest noise level is found when $\lambda = 0.0\text{ or } 1.0$, where on average the reward for a particular player always increases as any one player switch to the correct choice. 
In addition to the inherent noisiness of the game the feedback process itself may be noisy, typically modelled by adding Gaussian white noise to the rewards.

\begin{figure}[htbp]
\centering
\includegraphics[height=70mm,width=70mm]{images/goore_game}
\caption{Eight Player Goore Game}
\label{fig:gg}
\end{figure}

At the start of the game all participants select an action at random, as seen in figure~\ref{fig:gg}.
The referee tallies the votes and based on the performance criterion it calculates the reward each player receive.
The players will then use the reward they received from the referee to decide on whether to change their vote or to stick with it the next round.
The differing rewards in figure~\ref{fig:gg} are due to noisy feedback from the referee: the players generally do not receive the same reward in a given round.
This is part of the uncertainty in the Goore Game, that a player can make a good choice and still receive a low
reward. 
Another factor that makes the game difficult is that only the referee knows the performance criterion.
The players simply vote and receive a reward as feedback.

As the performance criterion is unimodal there is a unique fraction of yes-votes that yields the maximum reward.
For the remainder of this paper the performance criterion will be, as suggested earlier, the density of a Gaussian distribution, an example of which is given in figure~\ref{fig:gfunc}.
In the figure $\lambda$ is the fraction of players that vote yes and $\lambda^*$ is the target value, here 0.5, for which the referee provides the highest expected reward.
If the players reach the target value they are not ensured a high reward, due to the noisy environment, but the players all receive the highest possible expected reward by staying in this configuration.

\begin{figure}[htbp]
\centering
\begin{gnuplot}[terminal=epslatex,terminaloptions=color solid]
set style data lines
set nokey
set xlabel '$\lambda$'
set ylabel '$G(\lambda)$'
#set xrange[-0.25:1.25]
set yrange[0:*]
set label '$\lambda^*$' at 0.55, 0.6
set arrow from 0.5,2 to 0.5,0 nohead lt 0 lw 3
plot (1/(5*sqrt(2*pi))) * exp((-0.5)*(((x-0.5)/5)**2)) lw 3
\end{gnuplot}
\caption{A unimodal performance criterion}
\label{fig:gfunc}
\end{figure}

\section{Local Thompson sampling}
Local Thompson sampling (LTS) \cite{May2011} is based on a sampling technique pioneered by Thompson \cite{Thompson1933} in 1933. 
The method is Bayesian in nature, in that it uses new information to update probability estimates, but avoids the computational intractability that is often the drawback of Bayesian methods.
While the original method Thompson proposed was for choosing between 2 actions in a Bernoulli trial, it is easily generalised as a method of choosing among any number of actions with arbitrary reward distributions.
The \emph{local} part of the name is unimportant in our setting as the mean reward values are fixed, but it relates the fact that actions selection is done taking into account the current regressor (which is a value determining the mean reward for each action).

The algorithm functions as follows: For every available action it keeps a distribution estimate.
On every round $t$, it picks an action by drawing a random number from each distribution and selecting the action which achieved the highest value.
Using the reward $r_i$ which is then received, the arm estimate of the selected arm $i$ is updated.
Specifically, in the Gaussian bandit setting the mean and variance is calculated as follows \cite{Murphy2007}:
\begin{align*}
    \mu_i [t + 1] &= \frac{\sigma_i^2 [t] \cdot r_i + \sigma_{ob}^2 \cdot \mu_i [t]}{\sigma_i^2 [t] + \sigma_{ob}^2} \\
    \sigma_i^2 [t + 1] &= \frac{\sigma_i^2 [t] \sigma_{ob}^2}{\sigma_i^2 [t] + \sigma_{ob}^2} = \left(\frac{1}{\sigma_{ob}^2} + \frac{1}{\sigma_i^2 [t]} \right)^{-1} \text{,}
\end{align*}
where $\sigma_i^2 [t + 1]$ and $\mu_i [t + 1]$ is, respectively, the variance and mean of the updated arm.
The term $\sigma_{ob}$, the observation noise, is the parameter which is investigated in this report.
It is assigned a value akin to a learning rate: A low value signifies that new information should be given much importance, while a high value means that new observations are lightly weighted.
At the same time the \ob{} can be thought of as a discount factor, as it determines the rate of exploration versus exploitation.
This is illustrated in figure~\ref{fig:variance}. 
It shows how the variance of the estimates develops as a function of $t$, for the cases where $\sigma_{ob}^2 \in \{0.0001, 1.0\}$, with $\sigma_i [0] = 50$ for all $i$.
We see that the observation noise value determines how fast the variance shrinks and what it converges towards, and hence, how much time to allocate for exploration.

\begin{figure}[htbp]
    \centering
%% Haskell code to generate data %%
% let var v ob = let vv = (v*ob)/(v+ob) in vv : var vv ob
% let ppB b = unlines $ map ppL b
% let ppL (t,a,b,c) = show t ++ " " ++ show a ++ " " ++ show b ++ " " ++ show c
% writeFile "est.txt" . unlines . map ppB . transpose 
%   $ map (\(est, ob) -> (zip4 (var est ob) ([1..10000]) (repeat est) (repeat ob)))
%         [(est, ob) | est <- [2500],
%                      ob <- ([0.001,0.002, 0.003, 0.006, 0.01, 0.03,0.06] ++ [0.1,0.2..3.0])]
%
%% 3D plot %%
%    \begin{gnuplot}[terminal=epslatex,terminaloptions=color solid]
%    set grid
%    set ylabel "[r]{\\shortstack{Observation noise \\\\ ($\\sigma_{ob}^2$)}}" offset 6,-2
%    set xlabel 'Rounds' offset 0,-0.5
%    set zlabel 'Variance estimate ($\hat{\sigma}^2$)' rotate offset -0.5,0
%    set xyplane 0
%    set log z; set log cb
%    set xrange [0:1000]
%    set zrange [1e-06:3]
%    set ytics 0.4 offset 0,-0.5
%    set xtics offset 0,-0.5
%     set palette model CMY rgbformulae 7,5,15
%    splot 'est.txt' using 2:(column(4) < 1.5 ? column(4) : 1/0):(column(3) == 2500.0 ? column(1) : 1/0) every :::::1000 with pm3d title "" \
%    ,'est.txt' using 2:(column(4)<1.5?column(4):1/0):(column(3)==2500.0?column(1):1/0) every 2:100::50::1001 with line lt -1 lw 1 title "" \
%    ,'est.txt'       using 2:(column(4) < 1.5 ? column(4) : 1/0):(column(3) == 2500.0 ? column(1) : 1/0) every 2:10::::70  with line lt -1 lw 1 title ""
%    \end{gnuplot}
%    \input{ob-var-fig1.tex}
%
%% Regular 2D plot
\begin{gnuplot}[terminal=epslatex,terminaloptions=color solid]
    set style data lines
    set key ins vert
    set log y
    set xlabel "Rounds"
    set ylabel 'Variance estimate $\sigma^2$' rotate
    set xtics 1,100,1000
    set xrange [1:1000]
    set yrange [*:1]
    plot 'est.txt' using ($4==1.0?$2:NaN):1 title '$\sigma_{ob}^2 = 1.0$' lt 1 lw 3, 'est.txt' using ($4==1.0e-3?$2:NaN):1 title '$\sigma_{ob}^2 = 0.0001$' lt 3 lw 3
\end{gnuplot}
\caption{Variance as a function of time (at $t = 0$, $\sigma$ is $50$)}
\label{fig:variance}
\end{figure}


\section{UCB1-Tuned}
In addition to local Thompson sampling, we implemented the UCB1 algorithm for 
comparing optimised Thompson samplers.

UCB1 \cite{Auer02UCB1} (Upper Confidence Bound) is a well-known algorithm for multi-armed bandits.
It is popular due to being very simple and reasonably performant. In comparison 
to LTS, it does not require any constant parameters, and in place of a given 
start estimate for the available arms the fist $K$ rounds are dedicated to 
initialising the arm estimates by playing every arm once.

UCB1-Tuned is a version of UCB1 that takes into consideration the estimate variance in addition to the estimate mean.
This should cause it to work better in practice.
In the remainder of this report, UCB1 means this improved algorithm.

The UCB1-Tuned strategy is simply to play the arm with index $i$ such that
\begin{displaymath}
    i = \operatorname*{argmax}_{j \in \{ 1..K \}} \left(\hat{\mu}_j + 
    \sqrt{\frac{\ln{n}}{n_j} \min(\frac{1}{4},\hat{V}_j)}\right)
\end{displaymath}
where 
\begin{displaymath}
    \hat{V}_j = \hat{\sigma}_j^2 + \sqrt{\frac{2\ln{n}}{n_j}}\text{.}
\end{displaymath}

$K$ is the number of available arms, $n_j$ the numer of times arm $j$ has 
been played, $n$ is the total number of plays, and $\hat{\mu}_j 
=\frac{(\text{cumulative reward for arm j})}{n_j}$.

In figure~\ref{fig:regretob} we compare the performance of LTS and UCB1.
We see that the optimal value for the \ob{} indeed causes LTS to perform better than UCB1.
The figure also illustrates why, when using LTS, it is important to make a good choice of observation noise.
In this case we see that the total regret achieved increases rapidly when the value is set too low, while it increases slightly as we move past the optimum.
With different setups we see similar results, although the slope after the optimum may be steeper or closer to zero.

\begin{figure}[htbp]
    \hspace*{-0.8cm}
    \begin{minipage}[c]{0.39\textwidth}
    \begin{gnuplot}[terminal=epslatex,terminaloptions=color solid]
    set style data lines
    set xlabel "Observation noise"
    set ylabel "Total regret"
    set xtics 0.048
    set mxtics 2
    #set arrow from 0.24,5 to 0.24,15 nohead lt 0
    plot [0.048:0.445] [5:15] '../data/cumulative/good-5.0,0.5_bad-4.0,0.5_est-0.0,50.0_num-4_obnoise-0.24_rounds-1000_reps-10000_algo-LTS_kind-Cumulative.data' using 3:(5000-$2) title "LTS" lw 3, (5000-4992.961761728431) title "UCB1" lc 3 lw 3
    \end{gnuplot}
    \end{minipage}
    \hspace*{7.5cm}
    \begin{minipage}[c]{0.49\textwidth}
    \banditsetup{5.0}{4.0}{0.5}{4}{1000}
    \end{minipage}
\caption{Resulting total regret from varying observation noise.}
\label{fig:regretob}
\end{figure}

% \section{Sample stuff}
% Some simple and useful latex formatting.
% 
% \subsection{Quotations and citing}
% It is explained in detail in \cite[Ch.20]{Norvig03} that
% 
% \begin{quotation}
% \noindent \textit{``the true hypothesis eventually dominates the Bayesian 
% predication. For any fixed prior that does not rule out the true hypothesis, the 
% posterior probability of any false hypothesis will eventually vanish, simply 
% because the probability of generating ``uncharacteristic'' data indefinitely is 
% vanishingly small.''}
% \end{quotation}
% 
% \subsection{Figures}
% This distribution, and its probability density function, is displayed in Figure 
% \ref{fig:gaussian_distr_pdf}.
% \begin{figure}[ht]
%     \center\includegraphics[width=10cm]{images/normal_distr_pdf}	
%     \label{fig:gaussian_distr_pdf}
%     \caption{The Normal distribution PDF.}
% \end{figure}
% 
% \subsection{Equations}
% By using these probabilities, and Bayes formula, we can derive the Bayes 
% classifier.
% \begin{equation}
%     P(\omega_i | \boldsymbol{x}, \mathcal{X}) = \frac{p(\boldsymbol{x}|\omega_i, 
%     \mathcal(X))P(\omega_i|\mathcal{X})}{\sum_{j=1}^{c}p(\boldsymbol{x}|\omega_j, 
%     \mathcal{X})P(\omega_j|\mathcal{X})},
%     \label{eq:bayes_formula_1}
% \end{equation}
% 
% when we can separate the training samples by class into $c$ subsets 
% $\mathcal{X}_1, \ldots, \mathcal{X}_c$, with the samples in $\mathcal{X}_i$ 
% belonging to $\omega_i$.
% 

